# Copilot Instructions - Fine-tuning Acad√©mico

## üéØ Prop√≥sito del Proyecto

Fine-tuning de modelos preentrenados (Hugging Face Transformers) para clasificaci√≥n de textos acad√©micos. Objetivo: maximizar puntuaci√≥n en r√∫brica que valora **variedad de datasets (3+), experimentaci√≥n sistem√°tica (5+ configs), evaluaci√≥n exhaustiva y an√°lisis comparativo profundo**.

## üèóÔ∏è Arquitectura del Proyecto

### Estructura de Carpetas
```
scripts/          ‚Üí Scripts modulares de entrenamiento y evaluaci√≥n
notebooks/        ‚Üí Exploraciones y visualizaciones
data/            ‚Üí Datasets (.csv, .json) - 3 datasets diferenciados
models/          ‚Üí Modelos entrenados por configuraci√≥n
results/         ‚Üí Gr√°ficas, m√©tricas, an√°lisis comparativos
config.yaml      ‚Üí Hiperpar√°metros centralizados
```

### Flujo de Datos Principal
1. **Load Dataset** ‚Üí CSVs locales de talento estudiantil (Kaggle)
2. **Tokenize** ‚Üí `distilbert-base-uncased` tokenizer
3. **Train** ‚Üí `Trainer` API con m√∫ltiples configs (5+)
4. **Evaluate** ‚Üí Accuracy, F1, Precision, Recall, Confusion Matrix
5. **Compare** ‚Üí An√°lisis de impacto dataset + par√°metros
6. **Document** ‚Üí Resultados en tablas y gr√°ficas

## üîë Patrones Cr√≠ticos

### 1. Datasets Diferenciados (R√∫brica: Nivel 5)
Usar 3 datasets reales con caracter√≠sticas distintas:
- **Resume Screening** (https://www.kaggle.com/datasets/mfaisalqureshi/resume-screening-dataset) - 25 clases
- **Campus Recruitment** (https://www.kaggle.com/datasets/benroshan/campus-recruitment-data) - 2 clases
- **Student Performance** (https://www.kaggle.com/datasets/spscientist/students-performance-in-exams) - 3 clases

*Patr√≥n:* Cada dataset se carga desde `./data/*.csv` y se normaliza en `train.py`/`data_utils.py`.

### 2. Experimentaci√≥n Sistem√°tica (R√∫brica: Nivel 5)
M√≠nimo 5 configuraciones variando m√∫ltiples par√°metros:
```yaml
# En config.yaml, cada config_N define:
config_1: learning_rate=2e-5, batch_size=8, epochs=3
config_2: learning_rate=5e-5, batch_size=16, epochs=5
config_3: learning_rate=1e-4, batch_size=32, epochs=10
config_4: learning_rate=2e-5, batch_size=16, epochs=5, weight_decay=0
config_5: learning_rate=5e-5, batch_size=8, epochs=10, warmup_steps=1000, weight_decay=0.1
```
Con 3 datasets √ó 5 configs = **15 entrenamientos sistem√°ticos**.
*Patr√≥n:* Loop sobre configs en script `train.py` que crea directorio `/models/config_N/` para cada una.

### 3. Evaluaci√≥n Exhaustiva (R√∫brica: Nivel 5)
Para cada entrenamiento, computar:
- Accuracy, F1, Precision, Recall (via `evaluate` library)
- Matriz de confusi√≥n (via `sklearn.metrics.confusion_matrix`)
- Gr√°ficas de p√©rdida (train/eval loss por epoch)
- An√°lisis overfitting: comparar train vs eval metrics

*Patr√≥n:* Funci√≥n `compute_metrics()` integrada en `TrainingArguments` + post-procesamiento en `evaluate.py`.

### 4. An√°lisis Comparativo Profundo (R√∫brica: Nivel 5)
Guardar resultados en tablas CSV/JSON:
```
results/
‚îú‚îÄ‚îÄ config_comparison.csv  # Accuracy, F1 de cada config
‚îú‚îÄ‚îÄ dataset_impact.csv     # Impacto del tama√±o/tipo dataset
‚îî‚îÄ‚îÄ loss_curves/           # Gr√°ficas train/eval loss
```

*Patr√≥n:* Script `compare_results.py` que:
- Lee logs de todos los entrenamientos
- Crea tablas comparativas
- Genera gr√°ficas (matplotlib/seaborn)
- Analiza correlaciones (dataset size vs accuracy)

## üìã Developer Workflows

### 1. Agregar Nuevo Dataset (Real o Personalizado)
**Datasets reales (Kaggle + CSV local):**
```python
# En config.yaml dentro de conjuntos_datos:
nueva_dataset:
  nombre: "Nombre descriptivo"
  # Enlace: https://www.kaggle.com/datasets/...
  tama√±o_entrenamiento: 600
  tama√±o_prueba: 100
  num_etiquetas: 5
  descripcion: "Descripci√≥n..."

# En scripts/train.py, agregar caso en cargar_dataset():
elif self.nombre_dataset == "nueva_dataset":
    # Cargar CSV en ./data y normalizar columnas text/label
    ...
```

**Datasets personalizados (CSV local):**
```python
# Usar funci√≥n en scripts/data_utils.py:
conjunto = cargar_dataset_talento_desde_csv("resume_screening")
```

### 2. Entrenar Nueva Configuraci√≥n
```bash
python scripts/train.py \
  --archivo_config config.yaml \
  --conjunto_datos resume_screening \
  --modelo distilbert-base-uncased \
  --nombre_config config_1
```
*Patr√≥n:* Argparse con fallback a `config.yaml` si no se especifica.

### 3. Evaluar y Comparar
```bash
python scripts/evaluate.py --model_dir ./models/<modelo_entrenado> --conjunto_datos resume_screening
python scripts/compare_results.py --output_dir ./results
```

## üö´ Antipatrones a Evitar

‚ùå **No hacer:**
- Hardcodear paths absolutos (usar `config.yaml`)
- Usar single dataset sin variaci√≥n
- Sacar solo accuracy sin F1/precisi√≥n/recall
- Entrenar con 1-2 configuraciones fijas
- Sin control de random seeds (usar `seed=42` consistentemente)

‚úÖ **Hacer:**
- Todos los par√°metros en `config.yaml`
- Loops sobre m√∫ltiples datasets y configs
- M√©tricas completas + visualizaci√≥n
- 5+ configuraciones sistem√°ticas
- Reproducibilidad con seeds

## üì¶ Dependencias Clave

- `transformers==4.49.0` ‚Üí Modelos HF, Trainer API
- `datasets==3.5.0` ‚Üí Estructuras Dataset para entrenamiento
- `torch==2.6.0` ‚Üí Backend compu
- `evaluate==0.4.3` ‚Üí Calcular m√©tricas
- `scikit-learn==1.6.1` ‚Üí Matriz confusi√≥n, an√°lisis
- `matplotlib/seaborn` ‚Üí Gr√°ficas

## üéì Contexto Acad√©mico

**R√∫brica Key Criteria:**
- L5 (M√°xima) = 3+ datasets + 5+ configs + evaluaci√≥n exhaustiva + an√°lisis profundo
- L4 = 3 datasets + 4+ par√°metros + matrices + overfitting
- L3 = 2 datasets + 3 configs + m√©tricas + comparaci√≥n

*Este proyecto apunta a Nivel 5 en todas categor√≠as.*

## üí° Sugerencias para AI Agents

Cuando agregues c√≥digo:
1. **Prop√≥n** nuevas funciones siguiendo estructura de `scripts/`
2. **Valida** que use par√°metros desde `config.yaml`
3. **Incluir** logging de ejecuci√≥n en `/results/`
4. **Documentar** nuevos datasets agregados
5. **Proponer** gr√°ficas comparativas para `/results/`

---

**√öltima actualizaci√≥n:** Febrero 2026
